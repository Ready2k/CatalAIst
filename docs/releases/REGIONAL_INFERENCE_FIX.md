# Regional Inference Fix - Model Listing Issue

## üêõ Problem Identified

When "Use Regional Inference" is enabled in the frontend, model fetching fails with the error:
> "Failed to fetch models. Using default list. Check your credentials and permissions"

## üîç Root Cause

The issue was in the `createBedrockClient` method in `bedrock.service.ts`. The logic for handling regional inference endpoints was incorrect:

### Before (Broken Logic):
```typescript
if (config.useRegionalInference && config.regionalInferenceEndpoint) {
  // This was ignoring the custom endpoint and always using auto-generated
  const controlPlaneEndpoint = `https://bedrock.${region}.amazonaws.com`;
  clientConfig.endpoint = controlPlaneEndpoint;
} else if (config.useRegionalInference) {
  // This was only triggered when no custom endpoint was provided
  const controlPlaneEndpoint = `https://bedrock.${region}.amazonaws.com`;
  clientConfig.endpoint = controlPlaneEndpoint;
}
```

### After (Fixed Logic):
```typescript
if (config.useRegionalInference) {
  // Always use control plane endpoint for model listing, regardless of custom endpoint
  const region = config.awsRegion || 'us-east-1';
  const controlPlaneEndpoint = `https://bedrock.${region}.amazonaws.com`;
  clientConfig.endpoint = controlPlaneEndpoint;
}
```

## üîß Key Understanding

AWS Bedrock has two types of endpoints:

1. **Control Plane** (`https://bedrock.{region}.amazonaws.com`)
   - Used for: Model listing, account management, configuration
   - Required for: `ListFoundationModels` API call

2. **Runtime** (`https://bedrock-runtime.{region}.amazonaws.com`)
   - Used for: Model inference, chat completions
   - Required for: `InvokeModel` API call

The custom `regionalInferenceEndpoint` provided by users is for **runtime operations only**. For model listing, we must always use the control plane endpoint.

## ‚úÖ Changes Made

### 1. Fixed Endpoint Logic (`bedrock.service.ts`)
- Simplified the logic in `createBedrockClient` method
- Always use control plane endpoint when regional inference is enabled
- Custom endpoints are correctly used only for runtime operations

### 2. Added Debug Logging
- Added detailed logging in `listModels` method to show configuration
- Added logging in public routes to show received parameters
- Enhanced error reporting for troubleshooting

### 3. Created Debug Tools
- `debug-regional-inference.js` - Test script to verify the fix
- Enhanced logging to identify issues quickly

## üß™ How to Test the Fix

### 1. Start the Backend
```bash
cd CatalAIst/backend
npm run dev
```

### 2. Test with Debug Script
```bash
cd CatalAIst
node debug-regional-inference.js
```

### 3. Test in Frontend
1. Go to Configuration tab
2. Select AWS Bedrock provider
3. Enter valid AWS credentials
4. Enable "Use Regional Inference"
5. Click "Fetch Available Models"
6. Should now work without errors

### 4. Check Backend Logs
Look for these log entries:
```
[PublicRoutes] /models request: { useRegionalInference: true, ... }
[Bedrock] listModels called with config: { useRegionalInference: true, ... }
[Bedrock] Using control plane endpoint for model listing: https://bedrock.us-east-1.amazonaws.com
```

## üéØ Expected Behavior

### Before Fix:
- ‚ùå Regional inference enabled ‚Üí Model fetch fails
- ‚úÖ Regional inference disabled ‚Üí Model fetch works

### After Fix:
- ‚úÖ Regional inference enabled ‚Üí Model fetch works
- ‚úÖ Regional inference disabled ‚Üí Model fetch works
- ‚úÖ Custom regional endpoint ‚Üí Model fetch works (uses control plane for listing)

## üîí Security Notes

- Control plane endpoints are always HTTPS
- Regional inference respects AWS region settings
- Custom endpoints are validated for HTTPS usage
- All AWS credentials are handled securely

## üìù Additional Notes

- The fix maintains backward compatibility
- No changes needed to existing configurations
- Custom regional inference endpoints work for runtime operations
- Model listing always uses the appropriate control plane endpoint

## ‚ú® Result

Users can now:
1. ‚úÖ Enable regional inference without breaking model fetching
2. ‚úÖ Use custom regional inference endpoints for runtime operations
3. ‚úÖ Benefit from improved performance and compliance with regional endpoints
4. ‚úÖ See detailed logging for troubleshooting any remaining issues

The regional inference feature is now fully functional! üéâ